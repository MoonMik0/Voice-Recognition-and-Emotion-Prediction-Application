Voice Recognition and Emotion Analysis Project
In this project, we recorded the voices of four developers and applied various preprocessing techniques to these recordings. We split the audio files into smaller segments to create a large dataset. Then, we developed an AI model capable of distinguishing between these voices. We optimized the model for real-time performance, enabling it to predict which person is speaking as the audio is detected.

We also designed an interface where, once the recording starts, the system identifies the speaker in real time and displays a speaker icon beneath the photo of the identified person.

In the second phase, we recorded conversations where the four developers expressed different emotions (sad, happy, neutral, and angry), and preprocessed these recordings as well. Using this data, we developed a machine learning model capable of distinguishing emotions based on tone of voice and integrated it into the interface. Now, when the recording starts, the system not only predicts who is speaking but also performs real-time emotion analysis and displays it on the screen.

After the recording ends, the system generates a report displaying the transcribed text of what was said, along with the percentage of time each speaker talked compared to others. Additionally, the system performs emotion analysis on the words spoken and provides the percentages of emotions detected in each speaker’s conversation. Alongside this report, a pie chart is generated to visually represent the speaking time distribution for each speaker. 

Features:
Real-time voice recognition
Emotion analysis from voice tones
Interactive interface for speaker identification
Post-recording report with transcription and emotion distribution

This project devoloped by :
Özlem ELMALI
Ahmet Melih Kara
Muhammet Ali AĞDAŞ
Can ARI

This project is licensed under the MIT License - see the LICENSE file for details.
